#!/usr/bin/env python3

import requests
import sys
import argparse
from urllib3.exceptions import InsecureRequestWarning

# adding and parsing of arguments
parser = argparse.ArgumentParser(
    description="A simple script to detect (sub)domain caching",
    prog="detcache",
)

parser.add_argument(
    "subdomain_file",
    type=argparse.FileType("r"),
    help="the file of http (sub)domains",
)

parser.add_argument(
    "outfile",
    nargs="?",
    help="the file to store output to",
)

parser.add_argument(
    "-n",
    "--no-sslcheck",
    default=False,
    action="store_true",
    help="ignore that a url does not have valid ssl",
)

parser.add_argument(
    "-t",
    "--timeout",
    default=5,
    action="store",
    help="set a timeout for url connection",
)

parseArgs = parser.parse_args()

# list of most common cache headers and statuses
# (via personal testing, will look for more concise one in future)
cacheHeaderList = [
    "X-Cache",
    "Cf-Cache-Status",
    "Age",
    "X-Drupal-Cache",
    "X-Cache-Hits",
    "X-Drupal-Dynamic-Cache",
    "Cf-Ray",
    "Vary",
    "Strict-Transport-Security: max-age",
    "Varnish",
    "varnish",
    "Cloudflare",
    "Fastly",
]

# no way to do capital and lowercase, at least not without it being a pin in the ass
# any respected caching tool only uses full capital and first letter capital
cacheStatusList = ["HIT", "MISS", "STALE", "EXPIRED", "REVALIDATED", "UPDATING",
                   "Hit", "Miss", "Stale", "Expired", "Revalidated", "Updating"]

# only user agent at the moment but put in seperate variable in case we need more later
requestAddedHeaders = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}

# pushing all domains within file into readable list
userDomainList = [i.rstrip("\n") for i in parseArgs.subdomain_file]

# disabling insecure request warning if --no-sslcheck is used
if parseArgs.no_sslcheck:
    requests.packages.urllib3.disable_warnings(category=InsecureRequestWarning)


print("Starting....")
print("Finding cached domains....\n")

for url in userDomainList:
    try:
        # fixing the "did you mean: https://" thing
        if not url.startswith("http"):
            try:
                # requesting each url to retrieve headers
                # ignoring incorrect ssl cert depending on if arg is given
                with requests.get(
                    "https://" + url, verify=not parseArgs.no_sslcheck, timeout=int(parseArgs.timeout), headers=requestAddedHeaders
                ) as request:
                    responseHeaders = request.headers
            except:
                # same thing but in case of it not being ssl
                with requests.get(
                    "http://" + url, verify=not parseArgs.no_sslcheck, timeout=int(parseArgs.timeout), headers=requestAddedHeaders
                ) as request:
                    responseHeaders = request.headers
        else:
            # in case it already has http
            with requests.get(
                    url, verify=not parseArgs.no_sslcheck, timeout=int(parseArgs.timeout), headers=requestAddedHeaders
                ) as request:
                    responseHeaders = request.headers

        # much better readability than previous version
        # checks if any of the cache headers/statuses are in the response headers/values
        ifHeadersPresent = any(head in responseHeaders.keys() for head in cacheHeaderList)
        ifStatusPresent = any(head in responseHeaders.values() for head in cacheStatusList)

        # cant do cache attacks without a status, so check for both
        if ifHeadersPresent and ifStatusPresent:
            print(f"{url} - May Be Cached")

            # adding cached url to file if outfile is defined
            if parseArgs.outfile:
                with open(
                    parseArgs.outfile, "a"
                ) as cachedlist:
                    cachedlist.write(f"{url}\n")

        # catching exceptions and printing corresponding errors
    except requests.exceptions.HTTPError:
        print(f"{url} - HTTP Error")
        continue

    except requests.exceptions.SSLError:
        print(f"{url} - SSL Error (disable with -n)")
        continue

    except requests.exceptions.ConnectionError:
        print(f"{url} - Connection Error" )
        continue

    except requests.exceptions.Timeout:
        print(f"{url} - Timeout")
        continue

    except requests.exceptions.RequestException as e:
        print(e)
        continue
